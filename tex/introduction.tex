\section{Introduction}
\IEEEPARstart{R}{cent} cloud-based applications and online services increasingly require network service level objectives (SLOs) in terms of network bandwidth in order to ensure a targeted service quality~\cite{qiu2020firm,kannan2019grandslam,jang2015silo,sriraman2018mutune,jyothi2016morpheus,kumar2019picnic}.
As an example, a webserver that caters to $1,000,000$ monthly visitors, with an average of four-page views per user and an average data transfer of $10$MB per page, would necessitate an average network bandwidth of around $120$Mbps~\cite{loadninjahowto,abdelzaher2002performance, romanohowto}.
In particular, web servers often deliver HTTP-based video streaming services, where video quality is determined by the measured network bandwidth. Thus, insufficient bandwidth can lead to poor quality of service or significant playback delays.
Another major use case for network bandwidth requirements would be network functions virtualization~\cite{chintapalli2023ravin,tootoonchian2018resq,yen2020meeting, han2024byways, yoon2024mmtls, xu2024cyberstar}
%which must prevent performance interference between co-located virtual machines (VMs).
, which similarly involves processing large volumes of network packets at high rates, with performance requirements varying depending on specific network functions (NFs)~\cite{xu2024cyberstar}. It is critical for NFs to meet their target throughput (bandwidth), especially when multiple NFs are deployed across different VMs on the same physical server.

Given that VMs are a key element in cloud computing environments~\cite{chen2020alita,suo2018ananalysis}, the ability to meet the bandwidth requirements of VMs has emerged as a critical determinant of service quality within cloud computing~\cite{jang2015silo,kumar2019picnic}.
In addition, for user-facing microservice applications, latency is often a key determinant of service quality~\cite{park2021graf}. Recent studies~\cite{wang2024autothrottle,zhang2021sinan,qiu2020firm,park2021graf} have therefore focused on meeting network latency requirements by dynamically adjusting resource allocations for containers. 
Since these containers are typically deployed on VMs \kw{(including microVMs such as Firecracker of Amazon AWS)} rather than bare-metal servers~\cite{cai2023automan,wang2024autothrottle,entrialgo2024joint,bermejo2024goodness}, it is essential for VMs to be allocated sufficient CPU resources and network bandwidth to meet the network latency requirements.

\kw{Prior work has primarily pursued two directions—network scheduling and vCPU prioritization—to meet the bandwidth requirements. However, neither approach fully addresses meeting network bandwidth requirements in VMs. First, network scheduling~\cite{jang2015silo,jeyakumar2013eyeq,russell2008virtio} controls per-VM transmission rates via queueing and shaping, yet it implicitly assumes that the VM will receive \textbf{enough CPU time} to process packets at the desired rate; when CPU becomes the bottleneck (e.g., high packet-per-second traffic or CPU contention), the CPU scheduler—being oblivious to bandwidth requirements—can cap packet processing and prevent the target bandwidth from being reached even if rate limits are configured. 
Second, vCPU prioritization (including weight-based CPU sharing)~\cite{jia2018effectively,jia2020vsmt,xu2013vturbo,suo2017preserving} attempts to favor VMs with bandwidth requirements, but it does not directly answer the central question of \textit{how much CPU is needed for a given bandwidth target}. Without an accurate mapping, VMs have difficulty in meeting the bandwidth requirements accurately and efficiently.}

\kw{These limitations highlight three challenges in meeting network bandwidth SLOs for VMs. First, the CPU needed to sustain a target bandwidth is highly workload-, message-size-, and hardware-dependent, so a fixed Mbps→CPU rule can easily over- or under-provision. Second, bandwidth delivery depends on a coupled host–guest packet-processing pipeline (guest stack/virtio and host vhost/qemu/kernel), where the bottleneck can shift with contention, making naïve provisioning unstable. Third, real services are often multi-threaded and run in multi-vCPU VMs, introducing scaling and scheduling effects that complicate both accurate modeling and robust CPU enforcement for bandwidth requirements.}

\kw{This paper presents the design and implementation of \name, a general framework for translating network bandwidth SLOs into CPU allocations for VMs. \name tackles the three challenges above as follows. First, to handle the strong dependence of CPU demand on workload behavior, message size, and hardware, \name performs lightweight per-setting profiling: for each target workload specification (including message size and platform), it collects training samples by sweeping CPU allocations and measuring the resulting bandwidth, requiring up to 16 minutes in total for data collection and model building. Second, to account for the coupled host–guest packet-processing pipeline, \name explicitly separates their effects by learning two regression models, one for guest-side CPU usage and one for host-side CPU usage, and composes their predictions to estimate the CPU needed to sustain the target bandwidth. Third, to remain robust to multi-threaded services and varying numbers of vCPU, \name enforces allocations only at the host level (rather than controlling in-guest CPU usage). This enables \name to be independent of the guest’s internal threading model and vCPU configuration while still ensuring bandwidth requirements.}

\kw{More broadly, \name leverages ML to provide a fast and deployable bandwidth-to-CPU translation, but it differs from prior ML-based resource managers~\cite{qiu2020firm,zhang2021sinan,qiu2022simppo} that jointly optimize multiple resources (e.g., CPU, cache, memory, and disk) and therefore typically require higher-dimensional feature sets and longer training cycles. By narrowing the objective to bandwidth-to-CPU translation, \name can rely on a lightweight regression model that can be trained quickly for each new workload specification. Specifically, \name operates in three phases (\S\ref{subsec:framework}): (i) training data collection, where \name runs the target workload under a sweep of CPU allocations and records the achieved bandwidth to capture the bandwidth–CPU relationship; (ii) model training and CPU prediction, where \name trains a regression model from the collected data (we evaluate Linear Regression, Support Vector Regression, and Random Forest Regression, and select Random Forest Regression based on prediction accuracy in \S\ref{subsec:model}); and (iii) host-level CPU enforcement, where \name uses the predicted CPU requirement for the target bandwidth and applies the corresponding host-side CPU control to the VM.
Our evaluation shows that \name satisfies network bandwidth requirements consistently across diverse workloads, message sizes, and hardware configurations. Moreover, \name improves CPU allocation efficiency by up to 6.5$\times$ and achieves more stable bandwidth delivery—up to 3.0$\times$ lower bandwidth variation than existing schemes—for representative applications such as Webserver and Memcached.}

\kw{Compared to prior work, \name (1) formulates \emph{bandwidth-to-CPU translation} as the primary goal for VMs,
(2) decomposes prediction into \emph{guest and host CPU effects} to capture virtualization-specific packet-processing costs,
and (3) enforces only \emph{host-side CPU quotas} to remain robust to guest vCPU and threading configurations.
}

\kw{Our contributions are as follows:}

\begin{itemize}
    \item We design and implement \name, a general framework that translates per-VM bandwidth requirements into host CPU allocations.
    \item \name builds a workload- and environment-specific ML model with lightweight profiling, completing data collection and training in ${\sim}16$ minutes.
    \item \name models guest and host CPU effects separately, but enforces allocations only at the host to remain independent to the number of vCPUs.
    \item Across representative applications and traffic settings, \name improves CPU allocation efficiency by up to 6.5$\times$ and reduces bandwidth variation by up to 3.0$\times$ compared to existing schemes.
\end{itemize}

\begin{comment}
This paper presents the design and implementation of \name, a general framework for network bandwidth to CPU translations.
To cope with a variety of VM workloads and hardware configurations, \name leverages state-of-the-art machine learning (ML) techniques. Our key insight is that, unlike recent ML schemes~\cite{qiu2020firm,zhang2021sinan, qiu2022simppo}, which consider multiple resources, including cache, memory, and disk for resource allocation, focusing on CPU translation allows for the use of a simpler ML model. This enables \name to build a well-trained model quickly for diverse target workloads. When a new workload specification is received, \name operates in three main phases (\S\ref{subsec:framework}): (1) training data collection, (2) model training and CPU prediction, and (3) CPU allocation enforcement. During training data collection, \name measures the network bandwidth of the target workload under varying CPU allocations to identify the correlation between network bandwidth and CPU allocation.
Next, \name trains its ML model; after evaluating three representative regression models---Linear Regression, Support Vector Regression, and Random Forest Regression---we select Random Forest Regression due to its superior prediction accuracy (\S\ref{subsec:model}).
Lastly, \name predicts the appropriate CPU allocation required to meet the target network bandwidth using the trained model and enforces this allocation on the corresponding VMs (\S\ref{sec:eval}).
Note that \name targets VM-level CPU allocation for meeting network bandwidth requirements, which is different from recent studies for container-level CPU allocation for micro-services~\cite{qiu2020firm,cai2023automan,wang2024autothrottle}. As the containers run on VMs in cloud data centers, our work is complementary to the previous studies to offer proper CPU for both VMs and containers.

\name provides several key benefits as follows.
First, \name requires only ${\sim}16$ minutes to collect a dataset and train its ML model for new workloads; this enables quick adaptation to various workloads and server environments.
Second, \name requires no modifications inside VMs; this allows existing applications and guest OSes to operate without any modifications. This greatly helps the deployability of \name.
Third, \name improves CPU allocation efficiency by ${\sim}6.5\times$ compared to existing schemes. 
Finally, by collaborating with the work-conserving Linux CPU scheduler, \name dynamically redistributes underutilized CPU resources to other VMs, leading to higher overall system utilization.
We evaluate \name across diverse workloads and configurations, varying network bandwidth requirements, message sizes, and the number of concurrent requests. Our results demonstrate that \name achieves target network bandwidth more accurately, with ${\sim}3.0\times$ lower bandwidth variation compared to existing schemes, for widely used datacenter applications such as Webserver and Memcached 
\end{comment}